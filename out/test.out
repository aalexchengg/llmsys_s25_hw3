nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2024 NVIDIA Corporation
Built on Thu_Mar_28_02:18:24_PDT_2024
Cuda compilation tools, release 12.4, V12.4.131
Build cuda_12.4.r12.4/compiler.34097967_0
Requirement already satisfied: pycuda in /home/abcheng/miniconda3/envs/minitorch/lib/python3.10/site-packages (2024.1)
Requirement already satisfied: pytools>=2011.2 in /home/abcheng/miniconda3/envs/minitorch/lib/python3.10/site-packages (from pycuda) (2025.1.1)
Requirement already satisfied: appdirs>=1.4.0 in /home/abcheng/miniconda3/envs/minitorch/lib/python3.10/site-packages (from pycuda) (1.4.4)
Requirement already satisfied: mako in /home/abcheng/miniconda3/envs/minitorch/lib/python3.10/site-packages (from pycuda) (1.3.9)
Requirement already satisfied: platformdirs>=2.2 in /home/abcheng/miniconda3/envs/minitorch/lib/python3.10/site-packages (from pytools>=2011.2->pycuda) (4.3.6)
Requirement already satisfied: typing-extensions>=4.5 in /home/abcheng/miniconda3/envs/minitorch/lib/python3.10/site-packages (from pytools>=2011.2->pycuda) (4.12.2)
Requirement already satisfied: MarkupSafe>=0.9.2 in /home/abcheng/miniconda3/envs/minitorch/lib/python3.10/site-packages (from mako->pycuda) (3.0.2)
PyTorch Version: 2.6.0+cu124
CUDA Available: True
CUDA Version: 12.4
>>>>>>>>>>>>>>>>>>>>>>test_launch_attn_softmax, ntest [0], dtype [torch.float32]:
(batch_size, nhead, from_len, to_len, is_dec_self_attn, is_dec_self_attn_infer): (1, 8, 115, 282, False, False)
Run baseline...
Run custom...
Compare the results of custom and baseline...
Test passed. Time of custom/baseline (ms): 0.830 / 6.789, speedup: 8.176
>>>>>>>>>>>>>>>>>>>>>>test_launch_attn_softmax, ntest [1], dtype [torch.float32]:
(batch_size, nhead, from_len, to_len, is_dec_self_attn, is_dec_self_attn_infer): (2, 8, 229, 105, False, False)
Run baseline...
Run custom...
Compare the results of custom and baseline...
Test passed. Time of custom/baseline (ms): 0.968 / 9.259, speedup: 9.569
>>>>>>>>>>>>>>>>>>>>>>test_launch_attn_softmax, ntest [2], dtype [torch.float32]:
(batch_size, nhead, from_len, to_len, is_dec_self_attn, is_dec_self_attn_infer): (10, 8, 90, 55, False, False)
Run baseline...
Run custom...
Compare the results of custom and baseline...
Test passed. Time of custom/baseline (ms): 1.029 / 8.520, speedup: 8.279
>>>>>>>>>>>>>>>>>>>>>>test_launch_attn_softmax, ntest [3], dtype [torch.float32]:
(batch_size, nhead, from_len, to_len, is_dec_self_attn, is_dec_self_attn_infer): (1, 8, 430, 460, False, False)
Run baseline...
Run custom...
Compare the results of custom and baseline...
Test passed. Time of custom/baseline (ms): 2.394 / 27.569, speedup: 11.514
>>>>>>>>>>>>>>>>>>>>>>test_launch_attn_softmax, ntest [4], dtype [torch.float32]:
(batch_size, nhead, from_len, to_len, is_dec_self_attn, is_dec_self_attn_infer): (1, 8, 285, 164, False, False)
Run baseline...
Run custom...
Compare the results of custom and baseline...
Test passed. Time of custom/baseline (ms): 0.937 / 7.816, speedup: 8.345
Finished softmax fw test
>>>>>>>>>>>>>>>>>>>>>>test_launch_attn_softmax_bw, ntest [0], dtype [torch.float32]:
(batch_size, nhead, from_len, to_len): (1, 8, 115, 282)
Run baseline...
Run custom...
Compare the results of custom and baseline...
Test passed. Time of custom/baseline (ms): 20.803 / 5.518, speedup: 0.265
>>>>>>>>>>>>>>>>>>>>>>test_launch_attn_softmax_bw, ntest [1], dtype [torch.float32]:
(batch_size, nhead, from_len, to_len): (2, 8, 251, 143)
Run baseline...
Run custom...
Compare the results of custom and baseline...
Test passed. Time of custom/baseline (ms): 44.587 / 8.299, speedup: 0.186
>>>>>>>>>>>>>>>>>>>>>>test_launch_attn_softmax_bw, ntest [2], dtype [torch.float32]:
(batch_size, nhead, from_len, to_len): (9, 8, 105, 12)
Run baseline...
Run custom...
Compare the results of custom and baseline...
Test passed. Time of custom/baseline (ms): 8.677 / 2.462, speedup: 0.284
>>>>>>>>>>>>>>>>>>>>>>test_launch_attn_softmax_bw, ntest [3], dtype [torch.float32]:
(batch_size, nhead, from_len, to_len): (1, 8, 433, 31)
Run baseline...
Run custom...
Compare the results of custom and baseline...
Test passed. Time of custom/baseline (ms): 9.932 / 2.638, speedup: 0.266
>>>>>>>>>>>>>>>>>>>>>>test_launch_attn_softmax_bw, ntest [4], dtype [torch.float32]:
(batch_size, nhead, from_len, to_len): (4, 8, 96, 120)
Run baseline...
Run custom...
Compare the results of custom and baseline...
Test passed. Time of custom/baseline (ms): 29.013 / 5.529, speedup: 0.191
Finished softmax bw test
>>>>>>>>>>>>>>>>>>>>>>test_launch_layernorm, ntest [0], dtype [torch.float32]:
(batch_token_num, hidden_dim): (115, 32)
Run baseline...
Run custom...
Compare the results of custom and baseline...
Test passed. Time of custom/baseline (ms): 0.448 / 6.862, speedup: 15.333
>>>>>>>>>>>>>>>>>>>>>>test_launch_layernorm, ntest [1], dtype [torch.float32]:
(batch_token_num, hidden_dim): (502, 32)
Run baseline...
Run custom...
Compare the results of custom and baseline...
Test passed. Time of custom/baseline (ms): 0.470 / 6.984, speedup: 14.870
>>>>>>>>>>>>>>>>>>>>>>test_launch_layernorm, ntest [2], dtype [torch.float32]:
(batch_token_num, hidden_dim): (945, 32)
Run baseline...
Run custom...
Compare the results of custom and baseline...
Test passed. Time of custom/baseline (ms): 0.484 / 7.154, speedup: 14.774
>>>>>>>>>>>>>>>>>>>>>>test_launch_layernorm, ntest [3], dtype [torch.float32]:
(batch_token_num, hidden_dim): (433, 32)
Run baseline...
Run custom...
Compare the results of custom and baseline...
Test passed. Time of custom/baseline (ms): 0.453 / 6.758, speedup: 14.935
>>>>>>>>>>>>>>>>>>>>>>test_launch_layernorm, ntest [4], dtype [torch.float32]:
(batch_token_num, hidden_dim): (384, 32)
Run baseline...
Run custom...
Compare the results of custom and baseline...
Test passed. Time of custom/baseline (ms): 0.439 / 6.761, speedup: 15.386
Finished layernorm fw test
>>>>>>>>>>>>>>>>>>>>>>test_launch_layernorm_bw, ntest [0], dtype [torch.float32]:
(batch_token_num, hidden_dim): (115, 32)
Run baseline...
Run custom...
Compare the results of custom and baseline...
Test passed. Time of custom/baseline (ms): 2.611 / 10.387, speedup: 3.979
>>>>>>>>>>>>>>>>>>>>>>test_launch_layernorm_bw, ntest [1], dtype [torch.float32]:
(batch_token_num, hidden_dim): (502, 32)
Run baseline...
Run custom...
Compare the results of custom and baseline...
Test passed. Time of custom/baseline (ms): 3.036 / 10.792, speedup: 3.554
>>>>>>>>>>>>>>>>>>>>>>test_launch_layernorm_bw, ntest [2], dtype [torch.float32]:
(batch_token_num, hidden_dim): (945, 32)
Run baseline...
Run custom...
Compare the results of custom and baseline...
Test passed. Time of custom/baseline (ms): 3.585 / 11.588, speedup: 3.232
>>>>>>>>>>>>>>>>>>>>>>test_launch_layernorm_bw, ntest [3], dtype [torch.float32]:
(batch_token_num, hidden_dim): (433, 32)
Run baseline...
Run custom...
Compare the results of custom and baseline...
Test passed. Time of custom/baseline (ms): 2.940 / 10.534, speedup: 3.583
>>>>>>>>>>>>>>>>>>>>>>test_launch_layernorm_bw, ntest [4], dtype [torch.float32]:
(batch_token_num, hidden_dim): (384, 32)
Run baseline...
Run custom...
Compare the results of custom and baseline...
Test passed. Time of custom/baseline (ms): 2.880 / 10.302, speedup: 3.577
Finished layernorm bw test
Running without fused kernel...



{
    "data_size": {
        "train": 97976,
        "validation": 4512,
        "test": 100
    }
}
Running with fused kernel...



{
    "data_size": {
        "train": 97976,
        "validation": 4512,
        "test": 100
    }
}
All finished.
